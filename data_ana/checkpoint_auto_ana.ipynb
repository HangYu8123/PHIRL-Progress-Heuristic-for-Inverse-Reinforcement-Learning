{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 16:44:25.428741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-29 16:44:25.439199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-29 16:44:25.442644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 16:44:25.451421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 16:44:26.049737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "# from imitation.algorithms.adversarial.airl import AIRL\n",
    "from IRL_lib_mod.airl import AIRL\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from utils.irl_utils import make_vec_env_robosuite\n",
    "from utils.demostration_utils import load_dataset_to_trajectories\n",
    "import os\n",
    "import h5py\n",
    "import json\n",
    "from robosuite.controllers import load_controller_config\n",
    "from utils.demostration_utils import load_dataset_and_annotations_simutanously\n",
    "from utils.annotation_utils import read_all_json\n",
    "from imitation.util import logger as imit_logger\n",
    "import imitation.scripts.train_adversarial as train_adversarial\n",
    "import argparse\n",
    "import robosuite as suite\n",
    "import torch\n",
    "from utils.demostration_utils import load_data_to_h5py\n",
    "from utils.annotation_utils import write_to_json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hang/DHIRL_Progress/learning-with-progress\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = \"Lift\"\n",
    "horizon = 300\n",
    "\n",
    "\n",
    "notebook_path = os.getcwd()  # Get current working directory (where the notebook is running)\n",
    "notebook_path = os.path.abspath(notebook_path)  # Get absolute path of the notebook\n",
    "#- \"experiments\"\n",
    "project_path = notebook_path.split(\"data analysis\")[0]\n",
    "print(project_path)\n",
    "dataset_path = os.path.join(project_path, \"human-demo/lift/low_dim_v141_lift_ph.hdf5\")\n",
    "# Now, proceed with your h5py file operations\n",
    "f = h5py.File(dataset_path, 'r')\n",
    "env_meta = json.loads(f[\"data\"].attrs[\"env_args\"])\n",
    "make_env_kwargs = dict(\n",
    "    robots=\"Panda\",             # load a Sawyer robot and a Panda robot\n",
    "    gripper_types=\"default\",                # use default grippers per robot arm\n",
    "    controller_configs=env_meta[\"env_kwargs\"][\"controller_configs\"],   # each arm is controlled using OSC\n",
    "    has_renderer=True,                      # on-screen rendering\n",
    "    render_camera=\"frontview\",              # visualize the \"frontview\" camera\n",
    "    has_offscreen_renderer=True,           # no off-screen rendering\n",
    "    control_freq=20,                        # 20 hz control for applied actions\n",
    "    horizon=horizon,                            # each episode terminates after 200 steps\n",
    "    use_object_obs=True,                   # no observations needed\n",
    "    use_camera_obs=False,\n",
    "    reward_shaping=True,\n",
    ")\n",
    "SEED = 1\n",
    "\n",
    "env = suite.make(\n",
    "    env_name,\n",
    "    **make_env_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingPolicy(MlpPolicy):\n",
    "    def forward(self, obs: torch.Tensor, deterministic: bool = False):\n",
    "        global print_cnt\n",
    "        print_cnt += 1\n",
    "\n",
    "            # Get the action, value, and log probability from the parent class\n",
    "        actions, values, log_probs = super().forward(obs, deterministic)\n",
    "        if print_cnt % 2000 == 0:\n",
    "            print(f\"Actions: {actions[-1].detach().cpu().numpy()}\")\n",
    "                        # Convert actions to NumPy for easier processing\n",
    "            actions_np = actions.detach().cpu().numpy()\n",
    "            # Update total actions and count of positive last elements\n",
    "\n",
    "            positive_last = np.sum(actions_np[:, -1] > 0)\n",
    "            ratio = positive_last / actions_np.shape[0]\n",
    "            print(f\"Positive ratio: {ratio}\")\n",
    "\n",
    "        # Log the actions (you can adjust the logging as needed)\n",
    "\n",
    "\n",
    "        # Return the outputs as usual\n",
    "        return actions, values, log_probs\n",
    "\n",
    "def evaluate_policy_on_env(env, \n",
    "                           exp_name,\n",
    "                           checkpoint,\n",
    "                           evaluate_times=10, \n",
    "                           render=False):\n",
    "    custom_objects = {\n",
    "        'policy_class': CustomLoggingPolicy\n",
    "    }\n",
    "    reward_net_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    policy = PPO.load(f\"{project_path}/checkpoints/{exp_name}/{checkpoint}/gen_policy/model\", custom_objects=custom_objects, device = reward_net_device)\n",
    "    reward_net = torch.load(f\"{project_path}/checkpoints/{exp_name}/{checkpoint}/reward_train.pt\", map_location=reward_net_device)\n",
    "    reward_net.eval()\n",
    "    reward_net.to(reward_net_device)\n",
    "    reward_net_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    reward_net.to(reward_net_device)\n",
    "    global env_name \n",
    "    if env_name == \"Lift\":\n",
    "        obs_keys = [\"cube_pos\", \"robot0_eef_pos\", \"robot0_eef_quat\", \"robot0_gripper_qpos\"]\n",
    "    else:\n",
    "\n",
    "        obs_keys = [\"object-state\", \"robot0_eef_pos\", \"robot0_eef_quat\", \"robot0_gripper_qpos\"]\n",
    "    \n",
    "    env_rewards = []\n",
    "    correlations = []\n",
    "    normalized_pearson_correlations = []\n",
    "    normalized_spearman_correlations = []\n",
    "    success_cnt = 0\n",
    "    SEED = 1\n",
    "    for i in range(evaluate_times):\n",
    "        obs = env.reset()\n",
    "        obs = [obs[key] for key in obs_keys]\n",
    "        obs = np.concatenate(obs)\n",
    "        past_action = np.zeros(7)\n",
    "        done = False\n",
    "        cnt = 0\n",
    "        rewards= []\n",
    "        total_disc_rew = []\n",
    "        frames = []\n",
    "        while not done:\n",
    "            \n",
    "            #action, _states = policy.predict(obs)\n",
    "            action, _ = policy.predict(obs, deterministic=True)\n",
    "            cnt += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "                frame = env.render()\n",
    "                frames.append(frame)\n",
    "            obs = torch.tensor(obs).float().unsqueeze(0).to(reward_net_device)\n",
    "            obs = obs.cpu().detach().numpy()\n",
    "            # print(\"obs\", obs)   \n",
    "            \n",
    "            #action, _ = policy.predict(obs, deterministic=True)\n",
    "            action = action.squeeze()\n",
    "            #print(action)\n",
    "            # if cnt > 200:\n",
    "            #     action[6] = 1\n",
    "            #action = action.cpu().detach().numpy().squeeze()\n",
    "\n",
    "            next_obs, reward, next_done, info = env.step(action)\n",
    "            \n",
    "            next_obs = [next_obs[key] for key in obs_keys]\n",
    "            next_obs = np.concatenate(next_obs)\n",
    "            # # print(next_obs)\n",
    "            obs = torch.tensor(obs).float().unsqueeze(0).to(reward_net_device)\n",
    "            obs_tensor = obs.unsqueeze(0).to(reward_net_device).detach()\n",
    "            action_tensor = torch.tensor(action).float().unsqueeze(0).to(reward_net_device)\n",
    "            next_obs_tensor = torch.tensor(next_obs).float().unsqueeze(0).to(reward_net_device)\n",
    "            done = torch.tensor([0]).float().unsqueeze(0).to(reward_net_device)\n",
    "            # get the reward from the reward network\n",
    "            disc_rew = reward_net(obs_tensor, action_tensor, next_obs_tensor, done)\n",
    "            total_disc_rew.append(disc_rew.item())\n",
    "            rewards.append(reward)\n",
    "            # print(type(reward))\n",
    "            # print(type(disc_rew.item()))\n",
    "            obs = next_obs\n",
    "            past_action = action\n",
    "            #print(f\"Discriminator Reward: {disc_rew}\")\n",
    "            # if action[6] > 0:\n",
    "            #     print(f\"gripper action: {action[6]}\")\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "                #print(\"******************Success*********************\")\n",
    "            # print(\"done\", next_done)\n",
    "            # print(\"info\", info)\n",
    "            #env.render()\n",
    "            if next_done:\n",
    "                #print(\"yessssssss\")\n",
    "                if obs[2] > 0.84:\n",
    "                    success_cnt += 1\n",
    "                break\n",
    "       # video_path = os.path.join(video_dir, f\"episode_{i+1}.mp4\")\n",
    "        \n",
    "        # print(f\"Total Discriminator Reward: {sum(total_disc_rew)}\")\n",
    "        # print(f\"Total Reward: {sum(rewards)}\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Compute correlations\n",
    "        correlation = scipy.stats.spearmanr(rewards, total_disc_rew)\n",
    "        # normalized_pearson = scipy.stats.pearsonr(rewards_normalized, total_disc_rew_normalized)\n",
    "        # normalized_spearman = scipy.stats.spearmanr(rewards_normalized, total_disc_rew_normalized)\n",
    "        \n",
    "        #print(f\"Correlation (Spearman): {correlation[0]}\")\n",
    "        # print(f\"Normalized Pearson Correlation: {normalized_pearson[0]}\")\n",
    "        # print(f\"Normalized Spearman Correlation: {normalized_spearman[0]}\")\n",
    "        \n",
    "        correlations.append(correlation[0])\n",
    "        # normalized_pearson_correlations.append(normalized_pearson[0])\n",
    "        # normalized_spearman_correlations.append(normalized_spearman[0])\n",
    "\n",
    "        env_rewards.append(sum(rewards))\n",
    "\n",
    "        # imageio.mimwrite(video_path, frames, fps=20, codec='libx264')\n",
    "        # print(f\"Saved video for episode {i+1} at {video_path}\")\n",
    "\n",
    "    print(f\"Success Rate: {success_cnt}/{evaluate_times}\")\n",
    "    print(f\"Average Reward: {np.mean(env_rewards)}\")\n",
    "    print(f\"Average Correlation: {np.mean(correlations)}\")\n",
    "    print(f\"reward list: {env_rewards}\")\n",
    "    return success_cnt, env_rewards, correlations, policy, reward_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hang/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/hang/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_115420/3272723157.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reward_net = torch.load(f\"{project_path}/checkpoints/{exp_name}/{checkpoint}/reward_train.pt\", map_location=reward_net_device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0/5\n",
      "Average Reward: 84.3483989282034\n",
      "Average Correlation: -0.3734024815410696\n",
      "reward list: [88.09039829875572, 46.626451149831524, 86.60383100640115, 101.72740745654556, 98.69390672948299]\n",
      "*************************************************************************************************\n",
      "Checkpoint 410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hang/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/hang/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0/5\n",
      "Average Reward: 88.44610431709118\n",
      "Average Correlation: -0.5078838309809661\n",
      "reward list: [97.6577759462132, 40.33785708726892, 105.85819109302646, 107.58477005068633, 90.79192740826099]\n",
      "*************************************************************************************************\n",
      "Checkpoint 420\n",
      "Success Rate: 0/5\n",
      "Average Reward: 74.16809215576686\n",
      "Average Correlation: -0.11155386170957456\n",
      "reward list: [62.96053075763963, 37.64160591162272, 92.2355409556465, 92.24787338441843, 85.754909769507]\n",
      "*************************************************************************************************\n",
      "Checkpoint 430\n",
      "Success Rate: 0/5\n",
      "Average Reward: 83.13773346030405\n",
      "Average Correlation: 0.5530699674440827\n",
      "reward list: [88.82717862964564, 56.96685251397061, 77.8198490372119, 95.36254538823128, 96.71224173246083]\n",
      "*************************************************************************************************\n",
      "Checkpoint 440\n",
      "Success Rate: 0/5\n",
      "Average Reward: 73.14373045173889\n",
      "Average Correlation: -0.38777003555670386\n",
      "reward list: [100.06128575538293, 47.45898698065929, 34.549733143271105, 90.19601986667905, 93.45262651270204]\n",
      "*************************************************************************************************\n",
      "Checkpoint 450\n",
      "Success Rate: 0/5\n",
      "Average Reward: 54.59364254172309\n",
      "Average Correlation: 0.26639629329214765\n",
      "reward list: [109.78900872977727, 46.0884910486715, 31.35518234081205, 42.504534213552, 43.23099637580266]\n",
      "*************************************************************************************************\n",
      "Checkpoint 460\n",
      "Success Rate: 0/5\n",
      "Average Reward: 80.54595033562403\n",
      "Average Correlation: 0.2670377226413627\n",
      "reward list: [90.6203310060906, 48.945575874164874, 56.236808675832904, 91.95874229740572, 114.96829382462604]\n",
      "*************************************************************************************************\n",
      "Checkpoint 470\n",
      "Success Rate: 0/5\n",
      "Average Reward: 74.02636989494684\n",
      "Average Correlation: 0.22994568390780318\n",
      "reward list: [90.16448215355716, 49.390051076402706, 54.310485295871736, 77.38501986909003, 98.88181107981256]\n",
      "*************************************************************************************************\n",
      "Checkpoint 480\n",
      "Success Rate: 0/5\n",
      "Average Reward: 78.39553655695443\n",
      "Average Correlation: -0.5763542261580683\n",
      "reward list: [92.98908882421932, 66.84756307305437, 54.33470426957199, 83.69207677493688, 94.11424984298961]\n",
      "*************************************************************************************************\n",
      "Checkpoint 490\n",
      "Success Rate: 0/5\n",
      "Average Reward: 74.99418610598698\n",
      "Average Correlation: -0.6582272247469415\n",
      "reward list: [85.62114165345444, 82.63655814595856, 53.6664639179748, 65.9800176275474, 87.06674918499972]\n",
      "*************************************************************************************************\n",
      "Checkpoint 500\n",
      "Success Rate: 0/5\n",
      "Average Reward: 81.2794387309037\n",
      "Average Correlation: -0.7471311459016212\n",
      "reward list: [93.41778889663678, 63.08710632179879, 69.9858554465814, 83.46358877463292, 96.44285421486866]\n",
      "*************************************************************************************************\n",
      "Checkpoint 510\n",
      "Success Rate: 0/5\n",
      "Average Reward: 88.53335165013175\n",
      "Average Correlation: -0.3800070667451861\n",
      "reward list: [116.6782796265889, 46.46882880233, 42.4242369883171, 122.27661881279599, 114.81879402062681]\n",
      "*************************************************************************************************\n",
      "Checkpoint 520\n",
      "Success Rate: 1/5\n",
      "Average Reward: 107.36658670663442\n",
      "Average Correlation: -0.3524888859072604\n",
      "reward list: [65.57832538762422, 33.85442528611068, 192.28912406254915, 139.95267198036052, 105.15838681652745]\n",
      "*************************************************************************************************\n",
      "Checkpoint 530\n",
      "Success Rate: 0/5\n",
      "Average Reward: 34.3000387805951\n",
      "Average Correlation: -0.008759119545772734\n",
      "reward list: [35.1442881157544, 35.06146008547492, 30.005671482456965, 33.80069425636202, 37.488079962927195]\n",
      "*************************************************************************************************\n",
      "Checkpoint 540\n",
      "Success Rate: 0/5\n",
      "Average Reward: 24.331192702200305\n",
      "Average Correlation: -0.605092536580433\n",
      "reward list: [21.59088803214499, 32.45907213679066, 16.80748443756684, 22.89115572878623, 27.9073631757128]\n",
      "*************************************************************************************************\n",
      "Checkpoint 550\n",
      "Success Rate: 0/5\n",
      "Average Reward: 21.670420953741196\n",
      "Average Correlation: -0.5598255980622007\n",
      "reward list: [20.01607984161527, 33.90017183811786, 14.450612520162863, 18.251124349246677, 21.734116219563308]\n",
      "*************************************************************************************************\n",
      "Checkpoint 560\n",
      "Success Rate: 0/5\n",
      "Average Reward: 16.683770119433337\n",
      "Average Correlation: -0.5256857520639118\n",
      "reward list: [14.131553414383092, 23.13182069145926, 9.093062924977284, 14.793841370894867, 22.268572195452172]\n",
      "*************************************************************************************************\n",
      "Checkpoint 570\n",
      "Success Rate: 0/5\n",
      "Average Reward: 14.010174047793594\n",
      "Average Correlation: -0.35819144657162855\n",
      "reward list: [16.37898561829045, 12.350602411950673, 8.03956438003954, 17.962833556578722, 15.318884272108578]\n",
      "*************************************************************************************************\n",
      "Checkpoint 580\n",
      "Success Rate: 0/5\n",
      "Average Reward: 18.7690215109229\n",
      "Average Correlation: -0.30505654507272306\n",
      "reward list: [18.040938147612312, 19.643241608232604, 12.867648682063741, 19.318148344577047, 23.975130772128797]\n",
      "*************************************************************************************************\n",
      "Checkpoint 590\n",
      "Success Rate: 1/5\n",
      "Average Reward: 104.76012277857713\n",
      "Average Correlation: -0.28026860097065753\n",
      "reward list: [98.53189732361064, 106.73755077310611, 102.30792250047897, 109.40952776906718, 106.8137155266227]\n",
      "*************************************************************************************************\n",
      "Checkpoint 600\n",
      "Success Rate: 0/5\n",
      "Average Reward: 99.34089128249221\n",
      "Average Correlation: -0.012776941966021882\n",
      "reward list: [99.53889450476083, 104.98084400186745, 70.1254467333152, 109.6646084725305, 112.39466269998704]\n",
      "*************************************************************************************************\n",
      "Checkpoint 610\n",
      "Success Rate: 0/5\n",
      "Average Reward: 84.37401796460344\n",
      "Average Correlation: 0.06046120512450137\n",
      "reward list: [67.92947030066736, 100.22281037751073, 49.380871756796665, 93.18780230104139, 111.14913508700107]\n",
      "*************************************************************************************************\n",
      "Checkpoint 620\n",
      "Success Rate: 0/5\n",
      "Average Reward: 92.41519213286652\n",
      "Average Correlation: 0.03616146846076068\n",
      "reward list: [91.2775390347597, 92.03988623710092, 63.10590847290683, 111.55408382996168, 104.09854308960355]\n",
      "*************************************************************************************************\n",
      "Checkpoint 630\n",
      "Success Rate: 0/5\n",
      "Average Reward: 78.7733488582666\n",
      "Average Correlation: -0.5101972244136046\n",
      "reward list: [76.93536631448553, 60.16798274060842, 63.140306372586934, 103.10861631649414, 90.51447254715796]\n",
      "*************************************************************************************************\n",
      "Checkpoint 640\n",
      "Success Rate: 0/5\n",
      "Average Reward: 72.77466448412301\n",
      "Average Correlation: -0.7390691007677862\n",
      "reward list: [77.11304519640363, 61.71671895524686, 59.256407616947655, 91.4779712878742, 74.30917936414266]\n",
      "*************************************************************************************************\n",
      "Checkpoint 650\n",
      "Success Rate: 1/5\n",
      "Average Reward: 88.29613186946874\n",
      "Average Correlation: -0.4889410415051775\n",
      "reward list: [95.6130985397264, 73.21061048274672, 72.07544332744772, 106.11264941925957, 94.4688575781633]\n",
      "*************************************************************************************************\n",
      "Checkpoint 660\n",
      "Success Rate: 0/5\n",
      "Average Reward: 84.59346277180309\n",
      "Average Correlation: -0.2946548294981055\n",
      "reward list: [84.12856622960497, 74.13718428157969, 67.9653363168351, 80.75903546043317, 115.97719157056255]\n",
      "*************************************************************************************************\n",
      "Best checkpoint based on reward: 520\n",
      "Best checkpoint based on success: 520\n",
      "highest_score 107.36658670663442\n",
      "highest_success 1\n"
     ]
    }
   ],
   "source": [
    "checkpoint_start = 100\n",
    "checkpoint_end = 400\n",
    "exp_name = \"DPHIRL_lift_mh_adv_shaping\" \n",
    "\n",
    "best_checkpoint_reward = None\n",
    "highest_score = 0\n",
    "best_checkpoint_success = None\n",
    "highest_success = 0\n",
    "for i in range(checkpoint_start, checkpoint_end, 10):\n",
    "    print(f\"Checkpoint {i}\")\n",
    "    success_cnt, env_rewards, correlations, policy, reward_net = evaluate_policy_on_env(env, exp_name, str(i), evaluate_times=5, render=False)\n",
    "    if np.mean(env_rewards) > highest_score:\n",
    "        highest_score = np.mean(env_rewards)\n",
    "        best_checkpoint_reward = i\n",
    "    if success_cnt > highest_success:\n",
    "        highest_success = success_cnt\n",
    "        best_checkpoint_success = i\n",
    "    \n",
    "    print(\"*************************************************************************************************\")\n",
    "\n",
    "\n",
    "print(f\"Best checkpoint based on reward: {best_checkpoint_reward}\")\n",
    "print(f\"Best checkpoint based on success: {best_checkpoint_success}\")\n",
    "print(\"highest_score\", highest_score)\n",
    "print(\"highest_success\", highest_success)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
